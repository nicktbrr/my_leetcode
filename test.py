# -*- coding: utf-8 -*-
"""Assignment4_solution.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZAHYAswgBoQXlItqa9r1YlN7I2vuuuDc

# Introduction

This homework assignment aims to provide hands-on experience with three different approaches in natural language processing: RNN model training, prompting a pretrained language model, and fine-tuning a language model. The task is to classify True/False in text using the Boolq dataset.

## Data set

* Utilize the Boolq dataset.
* You will apply three approaches to classify **True** and **False** from textual data.
* More details about the dataset can be found at the provided link.

## Three approaches

In this assignment, you will apply three distinct NLP approaches to classify True/False from textual data. Each approach should be executable within the Google Colab environment.

# Install dependency (run this only one time)
"""

!pip install datasets
# download and unzip the dataset
!gdown 1Uhv07383o3qZAaRQr5oTdBHKZUIi6akE
!unzip boolq.zip

"""# Train a RNN model (>60% accuracy on val set for full credit)

* Introduction: Recurrent Neural Networks (RNNs) are powerful for sequence modeling and have been extensively used in NLP for tasks like text classification.
* Task: Train a RNN to classify True/False.
* Details: Implement and train an RNN using PyTorch. The architecture should include an embedding layer, one or more RNN layers, and a dense output layer for classification.
* Model Flexibility: You are free to choose or modify any RNN architecture (e.g., LSTM, GRU) as long as it is compatible with Colab.

## Load and prepare the dataset
"""

from datasets import load_dataset, DatasetDict
from transformers import AutoTokenizer
import torch
from torch.utils.data import DataLoader, random_split
import torch.nn as nn
from tqdm import tqdm

# Check if a GPU is available and choose device accordingly
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("You are currently using: ", device)

# Load and tokenize the dataset
def load_and_preprocess_data():
    dataset = DatasetDict.load_from_disk('/content/boolq/')
    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')

    def tokenize_function(examples):
        text = list(map(lambda a, b: str(a) + str(b), examples['question'], examples['passage']))
        return tokenizer(text, padding="max_length", truncation=True, max_length=128)

    tokenized_datasets = dataset.map(tokenize_function, batched=True)
    tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'answer'])

    # Splitting the training dataset into training and validation
    train_size = int(0.9 * len(tokenized_datasets['train']))
    val_size = len(tokenized_datasets['train']) - train_size
    train_dataset, val_dataset = random_split(tokenized_datasets['train'], [train_size, val_size])

    return train_dataset, val_dataset, tokenized_datasets['validation']

train_dataset, val_dataset, test_dataset = load_and_preprocess_data()


# Data loaders
train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, pin_memory=True)
validation_loader = DataLoader(val_dataset, batch_size=32, shuffle=False, pin_memory=True)
test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, pin_memory=True)

"""## Define the RNN model"""

# Define the RNN Classifier
class RNNClassifier(nn.Module):
    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim, rnn_type="GRU", num_layers=2, bidirectional=True, dropout=0.5):
        super(RNNClassifier, self).__init__()
        self.embedding = nn.Embedding(input_dim, embedding_dim)
        self.dropout = nn.Dropout(dropout)
        self.rnn = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers,
                          batch_first=True, bidirectional=bidirectional, dropout=dropout if num_layers > 1 else 0)
        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)

    def forward(self, text):
        embedded = self.dropout(self.embedding(text))
        output, _ = self.rnn(embedded)
        hidden = output[:, -1, :]  # Get the last hidden state
        return self.fc(hidden)

# Model parameters
tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')
INPUT_DIM = tokenizer.vocab_size
EMBEDDING_DIM = 100
HIDDEN_DIM = 256
OUTPUT_DIM = 2  # BoolQ has 2 classes (True/False)
model = RNNClassifier(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM).to(device)

criterion = nn.CrossEntropyLoss()
optimizer = torch.optim.Adam(model.parameters())

"""## Training and evaluation functions"""

# Training and evaluation functions
def train_model(model, data_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    for batch in tqdm(data_loader):
        batch = {k: v.to(device) for k, v in batch.items()}  # Move batch to the appropriate device
        optimizer.zero_grad()
        predictions = model(batch['input_ids'])
        loss = criterion(predictions, batch['answer'])
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(data_loader)

def evaluate_model(model, data_loader, criterion, device):
    model.eval()
    total_loss = 0
    total_correct = 0
    with torch.no_grad():
        for batch in data_loader:
            batch = {k: v.to(device) for k, v in batch.items()}  # Move batch to the appropriate device
            predictions = model(batch['input_ids'])
            loss = criterion(predictions, batch['answer'])
            total_loss += loss.item()
            preds = predictions.argmax(dim=1)
            total_correct += (preds == batch['answer']).sum().item()
    avg_loss = total_loss / len(data_loader)
    accuracy = total_correct / len(data_loader.dataset)
    return avg_loss, accuracy

"""## Main training loop"""

# Main training loop
num_epochs = 5
for epoch in range(num_epochs):
    train_loss = train_model(model, train_loader, optimizer, criterion, device)
    val_loss, val_accuracy = evaluate_model(model, validation_loader, criterion, device)
    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')

test_loss, test_accuracy = evaluate_model(model, test_loader, criterion, device)
print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')

"""# 2. Prompting a pretrained LM (>54% accuracy on val set for full credit)

* Introduction: Prompting involves adapting a pre-trained model to a specific task without extensive retraining, leveraging the model's existing knowledge.
* Task: Use zero-shot learning by prompting a pretrained language model.
* Details: Utilize a pre trained language model to generate predictions based on prompts. Craft three different prompts to evaluate how well the model can infer the correct emotion.
* Model Flexibility: Any pretrained model available via libraries like Hugging Faceâ€™s Transformers that runs on Google Colab can be used.

## Install dependency
"""

!pip install datasets
!pip install sentence-transformers

"""## Load data set and the pretrained language model"""

import torch
from datasets import load_dataset, DatasetDict
from transformers import AutoTokenizer, AutoModelForMaskedLM
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics import f1_score, accuracy_score
from sklearn.metrics.pairwise import cosine_similarity
from tqdm import tqdm

# Check device: GPU if available, otherwise CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

# Load the dataset
dataset = DatasetDict.load_from_disk('/content/boolq/')

# Initialize tokenizer and model for masked language model prediction
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')
model = AutoModelForMaskedLM.from_pretrained('distilbert-base-uncased').to(device)

"""## Prompt engineering"""

# Prepare the masked predictions in batches
unmasked = []
# Categories for Boolq
categories = ["True", "False"]

# Prompt design
prefix = "The statement is "
suffix = ". Given the fact: "

"""## Batch inference"""

'''
batch_size = 32

# Function to process batches for mask filling
def process_batch(text_batch, passage_batch):
    prompts = [passage + " " + prefix + '[MASK]' + suffix + " " + text for passage, text in zip(passage_batch, text_batch)]
    inputs = tokenizer(prompts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = outputs.logits
    masked_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]
    predicted_tokens = [tokenizer.decode(predictions[i, idx].argmax(dim=-1)).strip() for i, idx in enumerate(masked_index)]
    return predicted_tokens

for i in tqdm(range(0, len(dataset['validation']['question']), batch_size)):
    text_batch = dataset['validation']['question'][i:i+batch_size]
    passage_batch = dataset['validation']['passage'][i:i+batch_size]
    unmasked.extend(process_batch(text_batch, passage_batch))
'''

batch_size = 32

# Function to process batches for mask filling
def process_batch(text_batch):
    prompts = [prefix + '[MASK]' + suffix + " " + text for text in text_batch]
    inputs = tokenizer(prompts, return_tensors='pt', padding=True, truncation=True, max_length=128).to(device)
    with torch.no_grad():
        outputs = model(**inputs)
        predictions = outputs.logits
    masked_index = (inputs.input_ids == tokenizer.mask_token_id).nonzero(as_tuple=True)[1]
    predicted_tokens =[tokenizer.decode(predictions[i, idx].argmax(dim=-1)).strip() for i, idx in enumerate(masked_index)]
    return predicted_tokens

for i in tqdm(range(0, len(dataset['validation']['question']), batch_size)):
    text_batch = dataset['validation']['question'][i:i+batch_size]
    unmasked.extend(process_batch(text_batch))

"""## Back-mapping result to the pre-defined categories"""

# Initialize SentenceTransformer and function for back-mapping
matching_model = SentenceTransformer('bert-base-nli-mean-tokens').to(device)

def encode_batches(sentences, model, batch_size=64):
    embeddings = []
    for i in range(0, len(sentences), batch_size):
        batch = sentences[i:i+batch_size]
        batch_embeddings = model.encode(batch, convert_to_tensor=True, device=device)
        embeddings.append(batch_embeddings)
    return torch.cat(embeddings)

# Predict and evaluate in batches
prediction = []
for i in tqdm(range(0, len(unmasked), batch_size)):
    z_batch = unmasked[i:i+batch_size]
    x_batch = dataset['validation']['question'][i:i+batch_size]

    sentence_batch = []
    for z, x in zip(z_batch, x_batch):
        sentences = [prefix + cat + suffix + " " + x for cat in categories + [z]]
        sentence_batch.extend(sentences)

    # Encode all sentences at once for this batch
    sentence_embeddings = encode_batches(sentence_batch, matching_model, batch_size=512)  # Use a different batch size if necessary

    # Calculate predictions using cosine similarity
    num_categories = len(categories) + 1
    for j in range(len(z_batch)):
        start_idx = j * num_categories
        end_idx = start_idx + num_categories
        back_mapping = cosine_similarity(
            [sentence_embeddings[end_idx - 1].cpu().numpy()],
            sentence_embeddings[start_idx:end_idx - 1].cpu().numpy()
        )
        prediction.append(np.argmax(back_mapping))

"""## Evaluation"""

# Get the labels and evaluate
label = dataset['validation']['answer']
print('F1_macro: ', f1_score(prediction, label, average='macro'))
print('F1: ', f1_score(prediction, label, average=None))
print('Accuracy: ', accuracy_score(prediction, label))

"""# 3. Fine-tune a pretrained LM (>65% accuracy on val set for full credit)

* Introduction: Fine-tuning adjusts the weights of a pretrained model specifically to the task at hand, improving performance by adapting the model's deep knowledge to your specific dataset.
* Task: Fine-tune a pretrained model on the Boolq dataset.
* Details: Choose a transformer model and fine-tune it using the training split of the boolq dataset. Adjust the learning rate, batch size, and other hyperparameters as necessary.
* Model Flexibility: Any transformer-based model that is supported by the Google Colab environment can be used. Ensure the chosen model is manageable within the resource constraints of Colab.

"""

!pip install datasets
!pip install accelerate -U # Note this package requires to restart runtime/session

tokenized_datasets['train'][0]

tokenized_datasets['train'][1]

from datasets import load_dataset
from transformers import AutoTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments
import torch
from torch.utils.data import random_split

# Check if a GPU is available and choose device accordingly
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("You are currently using: ", device)

# Load the dataset
# dataset = load_dataset('boolq')
dataset = DatasetDict.load_from_disk('/content/boolq/')
dataset = dataset.rename_column('answer', 'label')

# Load a tokenizer
tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')

# Tokenize the dataset
def tokenize_function(examples):
    text = list(map(lambda a, b: str(a) + str(b), examples['question'], examples['passage']))

    return tokenizer(text, padding="max_length", truncation=True, max_length=128)

tokenized_datasets = dataset.map(tokenize_function, batched=True)
tokenized_datasets.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])

# Split the training set to create a validation set
train_size = int(0.9 * len(tokenized_datasets['train']))
val_size = len(tokenized_datasets['train']) - train_size
train_dataset, val_dataset = random_split(tokenized_datasets['train'], [train_size, val_size])

"""## Load the model and trainer setup"""

# Load the model
model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased', num_labels=4).to(device)

# Set training arguments
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=3,
    per_device_train_batch_size=128,
    per_device_eval_batch_size=256,
    warmup_steps=500,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    evaluation_strategy="epoch",
)

# Compute metrics
def compute_metrics(p):
    preds = p.predictions.argmax(-1)
    accuracy = (preds == p.label_ids).astype(float).mean()
    return {'accuracy': accuracy}

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
)

# Train the model
trainer.train()

# Evaluate the model
results = trainer.evaluate(tokenized_datasets['validation'])
print(results)